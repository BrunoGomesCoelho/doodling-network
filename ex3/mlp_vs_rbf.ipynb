{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ilNq9sScQLHR"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, log_loss\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "TESTING = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classe que representa uma MLP\n",
    "\n",
    "Implementado de uma maneira vetorizada por mini batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": [
     28,
     39,
     47,
     56,
     104
    ]
   },
   "outputs": [],
   "source": [
    "class MLP(): \n",
    "    def __init__(self, dimensions=[], momentum=0.9, classification=True, \n",
    "                 lr=0.1, testing=False, use_relu=False):\n",
    "        self.lr = lr\n",
    "        self.velocities = []\n",
    "        self.momentum = momentum\n",
    "        self.classification = classification\n",
    "        \n",
    "        # Initialize fully connected layers\n",
    "        self.layers = []\n",
    "        for idx, (input_dim, output_dim) in enumerate(dimensions, 1):\n",
    "            self.layers.append(self.Linear(input_dim, output_dim))\n",
    "            # For each linear layer, add a velocity term starting at 0\n",
    "            self.velocities.append(np.zeros((input_dim, output_dim)))\n",
    "            if idx != len(dimensions):\n",
    "                if use_relu:\n",
    "                    self.layers.append(self.ReLU())\n",
    "                else:\n",
    "                    self.layers.append(self.Sigmoid())\n",
    "            else: # Last layer\n",
    "                if self.classification:\n",
    "                    self.layers.append(self.Softmax()) # for the last use softmax\n",
    "            \n",
    "        if classification:\n",
    "            self.cost = self.CrossEntropy()\n",
    "        else:\n",
    "            self.cost = self.SquareLoss()\n",
    "        \n",
    "    class Sigmoid():\n",
    "        def forward(self, x):\n",
    "            # Cip the sigmoid to avoid overflow\n",
    "            # See https://stackoverflow.com/questions/23128401/overflow-error-in-neural-networks-implementation\n",
    "            clipped_x = np.clip(x, -500, 500)\n",
    "            self.old_y = np.exp(clipped_x) / (1. + np.exp(clipped_x))\n",
    "            return self.old_y\n",
    "\n",
    "        def backward(self, grad):\n",
    "            return self.old_y * (1. - self.old_y) * grad\n",
    "\n",
    "    class Softmax():\n",
    "        def forward(self,x):\n",
    "            self.old_y = np.exp(x) / np.exp(x).sum(axis=1) [:,None]\n",
    "            return self.old_y\n",
    "\n",
    "        def backward(self,grad):\n",
    "            return self.old_y * (grad -(grad * self.old_y).sum(axis=1)[:,None])\n",
    "\n",
    "    class CrossEntropy():\n",
    "        def forward(self,x,y):\n",
    "            self.old_x = x.clip(min=1e-8,max=None)\n",
    "            self.old_y = y\n",
    "            return (np.where(y==1,-np.log(self.old_x), 0)).sum(axis=1)\n",
    "\n",
    "        def backward(self):\n",
    "            return np.where(self.old_y==1,-1/self.old_x, 0)\n",
    "    \n",
    "    class Linear():\n",
    "        def __init__(self,n_in,n_out):\n",
    "            self.weights = np.random.randn(n_in,n_out) * np.sqrt(2/n_in)\n",
    "            self.biases = np.zeros(n_out)\n",
    "\n",
    "            self.w_vel = np.zeros((n_in,n_out))\n",
    "            self.b_vel = np.zeros(n_out)\n",
    "\n",
    "        def update_velocity(self, w_vel, b_vel):\n",
    "            self.w_vel = w_vel\n",
    "            self.b_vel = b_vel\n",
    "\n",
    "        def forward(self, x):\n",
    "            self.old_x = x\n",
    "            return np.dot(x,self.weights) + self.biases\n",
    "\n",
    "        def backward(self,grad):\n",
    "            self.grad_b = grad.mean(axis=0)\n",
    "            self.grad_w = (np.matmul(self.old_x[:,:,None],grad[:,None,:])).mean(axis=0)\n",
    "            return np.dot(grad,self.weights.transpose())\n",
    "\n",
    "    def forward(self,x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def loss(self,x,y):\n",
    "        return self.cost.forward(self.forward(x),y)\n",
    "\n",
    "    def backward(self):\n",
    "        grad = self.cost.backward()\n",
    "        for i in range(len(self.layers)-1,-1,-1):\n",
    "            grad = self.layers[i].backward(grad)\n",
    "            \n",
    "    def fit(self, x, y, n_iter=100, mini_batch=32):\n",
    "        for _ in range(n_iter):   \n",
    "            total_loss = 0\n",
    "            for start_idx in range(0, len(x), mini_batch):\n",
    "                x_batch = x[start_idx:start_idx+mini_batch]\n",
    "                y_batch = y[start_idx:start_idx+mini_batch]   \n",
    "                \n",
    "                # Forward and backward\n",
    "                loss = self.loss(x_batch,y_batch)\n",
    "                total_loss += loss.sum()\n",
    "                self.backward()\n",
    "                \n",
    "                # Update according to momentum\n",
    "                for layer in self.layers:\n",
    "                    if type(layer) == self.Linear:\n",
    "                        new_w_vel = self.momentum*layer.w_vel - self.lr*layer.grad_w\n",
    "                        new_b_vel = self.momentum*layer.b_vel - self.lr*layer.grad_b\n",
    "                        \n",
    "                        layer.weights += new_w_vel\n",
    "                        layer.weights += new_b_vel\n",
    "                        \n",
    "                        layer.update_velocity(new_w_vel, new_b_vel)\n",
    "\n",
    "    def predict(self, x):\n",
    "        output = self.forward(x)\n",
    "        if self.classification:\n",
    "            return output.argmax(axis=-1)                       \n",
    "        else:\n",
    "            return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classe que representa uma RBF\n",
    "\n",
    "Implementado de uma maneira vetorizada por mini batch. \n",
    "\n",
    "Primeiro vamos definir as diversas funções de RBF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian(X, centroid, sigma=None):\n",
    "    norm = np.linalg.norm(X-centroid, axis=1)\n",
    "    mult = -1 / (2*sigma**2)\n",
    "    return np.exp(mult*norm**2)\n",
    "    \n",
    "def multi_square(X, centroid, sigma):\n",
    "    norm = np.linalg.norm(X-centroid, axis=1)\n",
    "    return np.sqrt(sigma**2 + norm**2)\n",
    "\n",
    "def thin_plate_spline(X, centroid, sigma):\n",
    "    norm = np.linalg.norm(X-centroid, axis=1)\n",
    "    return norm*norm*np.log(norm[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora sim a nossa RBF que chama uma Adalaide simples por trás:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class RBF(): \n",
    "    def __init__(self, rbf, input_size=3, output_size=3, lr=0.1, \n",
    "                 testing=False):\n",
    "        self.lr = lr\n",
    "        self.mlp = MLP(dimensions=[(input_size, output_size)], momentum=0,\n",
    "                      classification=True, lr=lr, testing=testing)\n",
    "        \n",
    "        # Initialize fully connected layers\n",
    "        self.centroids = None\n",
    "        self.rbf = rbf\n",
    "           \n",
    "    def calculate_sigma(self):\n",
    "        \"\"\"Calculate sigma according to:  sigma = Dmax / sqrt(2*K)\n",
    "        where Dmax is the maximum dist between centroids and K the number of centroids\n",
    "        \"\"\"\n",
    "        D_max = 0\n",
    "        for idx, group1 in enumerate(self.centroids):\n",
    "            for group2 in self.centroids[idx+1:]:\n",
    "                dist = np.linalg.norm(group1-group2)\n",
    "                if dist > D_max:\n",
    "                    D_max = dist\n",
    "        return D_max / np.sqrt(2*len(self.centroids))\n",
    "        \n",
    "    def rbf_layer(self, x):\n",
    "        \"\"\"Converts our x matrix, where each row is a example and each column a feature,\n",
    "        into a new matrix where each row is a example and each columm a\n",
    "        RBF function with a different centroid.\"\"\"\n",
    "        new_x = np.zeros((x.shape[0], len(self.centroids)))\n",
    "        \n",
    "        for idx, centroid in self.centroids.iterrows():\n",
    "            new_x[:, idx-1] = self.rbf(x, centroid, self.sigma)\n",
    "        return new_x\n",
    "\n",
    "    def fit(self, x, y, n_iter=100, mini_batch=32):\n",
    "        # Calculate the centroids and sigma\n",
    "        self.centroids = x.groupby(y).mean()\n",
    "        self.sigma = self.calculate_sigma()\n",
    "        \n",
    "        # Go through the RBF layer\n",
    "        new_x = self.rbf_layer(x)\n",
    "        new_y = pd.get_dummies(y).values\n",
    "        \n",
    "        # Call the 1 layer MLP\n",
    "        self.mlp.fit(new_x, new_y, n_iter=n_iter)\n",
    "                \n",
    "    def predict(self, x):\n",
    "        new_x = self.rbf_layer(x)\n",
    "        output = self.mlp.forward(new_x)\n",
    "        return output.argmax(axis=-1)                       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- - - \n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funções auxiliares\n",
    "\n",
    "Diversas funções que seram utilizadas para ambos os datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Função para carregar os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def load_and_analyze(filename, target, header=0):\n",
    "    df = pd.read_csv(filename, header=header)\n",
    "    # Check for nulls\n",
    "    print(f\"Temos {df.isna().sum().sum()} nulos na nossa tabela\")\n",
    "    \n",
    "    x = df.drop(columns=[target])\n",
    "    y = df[target] - 1\n",
    "    sns.countplot(y)\n",
    "    plt.show()\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temos 0 nulos na nossa tabela\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x, y = load_and_analyze(\"wine.data\", target=0, header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Função  para normalizar os dados que vamos utilizar posteriormente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(x_train, x_test, y=None, pandas=True):\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    x_train_norm = scaler.fit_transform(x_train)\n",
    "    x_test_norm = scaler.transform(x_test)\n",
    "    x_train_norm, x_test_norm = [pd.DataFrame(x) for x in (x_train_norm, x_test_norm)]\n",
    "    \n",
    "    return x_train_norm, x_test_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Função para rodar 1 experimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def run_experiment(x, y, model, test_percent=0.3, n_iter=100, mlp=False):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Process our data and normalize\n",
    "    if mlp:\n",
    "        new_y = pd.get_dummies(y)\n",
    "    else:\n",
    "        new_y = y\n",
    "        \n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, new_y, stratify=y, \n",
    "                                                       test_size=test_percent)\n",
    "    x_train, x_test = normalize_data(x_train, x_test, pandas=True)\n",
    "\n",
    "    # Train and evaluate\n",
    "    if mlp:\n",
    "        model.fit(x_train.values, y_train.values, n_iter=n_iter)\n",
    "    else:\n",
    "        model.fit(x_train, y_train.values, n_iter=n_iter)\n",
    "    preds = model.predict(x_test)\n",
    "    \n",
    "    if mlp:\n",
    "        score = accuracy_score(y_test.values.argmax(axis=-1), preds)\n",
    "    else:\n",
    "        score = accuracy_score(y_test, preds)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    return score, end_time-start_time\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Roda todos os nossos experimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"Nome\", \"Acurácia\", \"Tempo\"]\n",
    "results = pd.DataFrame(columns=cols)\n",
    "\n",
    "models = {\n",
    "    \"RBF-G\": RBF(gaussian),\n",
    "    \"RBF-MQ\": RBF(multi_square),\n",
    "    \"RBF-TPS\": RBF(thin_plate_spline),\n",
    "    \"MLP-0\": MLP(dimensions=[(13, 3)], momentum=0, lr=0.1),\n",
    "    \"MLP-1\": MLP(dimensions=[(13, 32), (32, 3)], momentum=0, lr=0.1),\n",
    "    \"MLP-2\": MLP(dimensions=[(13, 16), (16, 8), (8, 3)], momentum=0, lr=0.1),\n",
    "}\n",
    "is_mlp = [False]*3 + [True]*3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 10000\n",
    "for (name, model), mlp in zip(models.items(), is_mlp):\n",
    "    score, runtime = run_experiment(x, y, model, n_iter=n_iter, mlp=mlp)\n",
    "    results.loc[(len(results))] = (name, score, runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format the data to be generated in our report, using https://www.tablesgenerator.com/\n",
    "for index, data in results.iterrows():\n",
    "    name, acc, runtime = data \n",
    "    print(f\"{name}\\t{100*acc:.2f}%\\t{runtime:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "adaline.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5rc1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
